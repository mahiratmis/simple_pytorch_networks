{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('p37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ab66cd2a7eef53324163067b08cf46878006a1ba8ec8ccae931ca78a06f8215e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<img src=\"images/vanilla_gan.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptod from https://debuggercafe.com/generating-mnist-digit-images-using-vanilla-gan-with-pytorch/\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib   # plotting purposes\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x.view(-1, 1, 28, 28)   # Bx1x28x28\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 1024),   #784 is MNIST input feature dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 1),      # single value stating fake or real\n",
    "            nn.Sigmoid()            # value between 0 and 1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)         # reshape input same as torch.flatten(x, start_dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch parameters\n",
    "SEED = 60            # reproducability\n",
    "# NN Parameters\n",
    "EPOCHS = 300         # number of epochs\n",
    "LR = 0.0002          # learning rate\n",
    "MOMENTUM = 0.9       # momentum for the optimizer\n",
    "WEIGHT_DECAY = 1e-5  # weight decay for the optimizer\n",
    "GAMMA = 0.1          # learning rate schedular\n",
    "BATCH_SIZE = 512     # number of images to load per iteration\n",
    "# GAN parameters\n",
    "SAMPLE_SIZE = 64     # number of fake images to sample\n",
    "LATENT_SIZE = 128    # size of latent or noise vector\n",
    "DISC_STEPS  = 1      # number of steps to apply to the discriminator\n",
    "\n",
    "# manual seed to reproduce same results\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# DOWNLOADING AND LOADING MNIST DATASET \n",
    "mnist_folder= './data'\n",
    "\n",
    "# normalize each image and set the pixel values between -1 and 1\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "# download the dataset if not already downloaded and set necessery transforms\n",
    "tr_dataset   = MNIST(mnist_folder, train=True, download=True, transform=img_transform)\n",
    "# prepare loader for the training dataset\n",
    "train_loader = torch.utils.data.DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "\n",
    "# determine where to run the code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# create the generator network and move to gpu if available\n",
    "gen_net = Generator(LATENT_SIZE).to(device)\n",
    "# create the discriminator and move to gpu if available\n",
    "disc_net = Discriminator().to(device)\n",
    "\n",
    "# specify the loss to be used\n",
    "# Binary Cross Entropy Loss\n",
    "loss_fn = nn.BCELoss()\n",
    "# specify the optimizer for generator network\n",
    "optimizer_gen = optim.Adam(gen_net.parameters(), lr=LR)\n",
    "# specify the optimizer for discriminator network\n",
    "optimizer_disc = optim.Adam(disc_net.parameters(), lr=LR)\n",
    "\n",
    "# function to create the noise vector\n",
    "def create_noise(batch_size):\n",
    "    return torch.randn(batch_size, LATENT_SIZE).to(device)\n",
    "\n",
    "\n",
    "def train_discriminator(real_x, fake_x):\n",
    "    B = real_x.size(0)                         # batch size\n",
    "    real_y = torch.ones(B, 1).to(device)       # create labels of 1\n",
    "    fake_y = torch.zeros(B, 1).to(device)      # create labels of 0\n",
    "    \n",
    "    optimizer_disc.zero_grad()\n",
    "    # forward pass the real data\n",
    "    output_real = disc_net(real_x)\n",
    "    # estimate loss         \n",
    "    loss_real = loss_fn(output_real, real_y)\n",
    "    # forward pass the fake data\n",
    "    output_fake = disc_net(fake_x)\n",
    "    # estimate loss\n",
    "    loss_fake = loss_fn(output_fake, fake_y)\n",
    "    # accumulate gradients for both passes \n",
    "    loss_real.backward()\n",
    "    loss_fake.backward()\n",
    "    # update weights of discriminator\n",
    "    optimizer_disc.step()\n",
    "    return loss_real.item() + loss_fake.item()\n",
    "\n",
    "# update generator weights using the gradients of discriminator\n",
    "def train_generator(fake_x):\n",
    "    B = fake_x.size(0)                       # batch size\n",
    "    real_y = torch.ones(B, 1).to(device)     # create labels of 1\n",
    "\n",
    "    optimizer_gen.zero_grad()\n",
    "    # forward pass the fake data on discriminator\n",
    "    output = disc_net(fake_x)\n",
    "    # determine how far we are from real label\n",
    "    loss = loss_fn(output, real_y)\n",
    "    # calculate gradients\n",
    "    loss.backward()\n",
    "    # update generator weights\n",
    "    optimizer_gen.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the images generated by the generator\n",
    "def save_generator_image(image, path):\n",
    "    save_image(image, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Generator loss: 3.85858167, Discriminator loss: 0.28020229\n",
      "Epoch 37 of 300\n",
      "Generator loss: 3.49907235, Discriminator loss: 0.35612976\n",
      "Epoch 38 of 300\n",
      "Generator loss: 3.56890581, Discriminator loss: 0.32570238\n",
      "Epoch 39 of 300\n",
      "Generator loss: 3.41493368, Discriminator loss: 0.39599920\n",
      "Epoch 40 of 300\n",
      "Generator loss: 3.28369769, Discriminator loss: 0.36247623\n",
      "Epoch 41 of 300\n",
      "Generator loss: 3.34816973, Discriminator loss: 0.34540916\n",
      "Epoch 42 of 300\n",
      "Generator loss: 3.14941542, Discriminator loss: 0.38168964\n",
      "Epoch 43 of 300\n",
      "Generator loss: 3.04987662, Discriminator loss: 0.38107483\n",
      "Epoch 44 of 300\n",
      "Generator loss: 2.95842652, Discriminator loss: 0.40814742\n",
      "Epoch 45 of 300\n",
      "Generator loss: 3.03955083, Discriminator loss: 0.39038319\n",
      "Epoch 46 of 300\n",
      "Generator loss: 2.97063884, Discriminator loss: 0.41950736\n",
      "Epoch 47 of 300\n",
      "Generator loss: 3.05880041, Discriminator loss: 0.43588309\n",
      "Epoch 48 of 300\n",
      "Generator loss: 2.89793772, Discriminator loss: 0.45019102\n",
      "Epoch 49 of 300\n",
      "Generator loss: 2.75017113, Discriminator loss: 0.47084901\n",
      "Epoch 50 of 300\n",
      "Generator loss: 2.72909124, Discriminator loss: 0.47541634\n",
      "Epoch 51 of 300\n",
      "Generator loss: 2.67404162, Discriminator loss: 0.47504954\n",
      "Epoch 52 of 300\n",
      "Generator loss: 2.56208398, Discriminator loss: 0.50274705\n",
      "Epoch 53 of 300\n",
      "Generator loss: 2.46349461, Discriminator loss: 0.54750101\n",
      "Epoch 54 of 300\n",
      "Generator loss: 2.51100743, Discriminator loss: 0.51081404\n",
      "Epoch 55 of 300\n",
      "Generator loss: 2.35637997, Discriminator loss: 0.58081266\n",
      "Epoch 56 of 300\n",
      "Generator loss: 2.40152877, Discriminator loss: 0.57503612\n",
      "Epoch 57 of 300\n",
      "Generator loss: 2.25415511, Discriminator loss: 0.61254650\n",
      "Epoch 58 of 300\n",
      "Generator loss: 2.20905769, Discriminator loss: 0.62130140\n",
      "Epoch 59 of 300\n",
      "Generator loss: 2.33388033, Discriminator loss: 0.59331069\n",
      "Epoch 60 of 300\n",
      "Generator loss: 2.15416124, Discriminator loss: 0.63868267\n",
      "Epoch 61 of 300\n",
      "Generator loss: 2.12652361, Discriminator loss: 0.64093366\n",
      "Epoch 62 of 300\n",
      "Generator loss: 2.13212096, Discriminator loss: 0.63575202\n",
      "Epoch 63 of 300\n",
      "Generator loss: 2.22397191, Discriminator loss: 0.61996719\n",
      "Epoch 64 of 300\n",
      "Generator loss: 2.10094983, Discriminator loss: 0.66368866\n",
      "Epoch 65 of 300\n",
      "Generator loss: 2.06846194, Discriminator loss: 0.65805800\n",
      "Epoch 66 of 300\n",
      "Generator loss: 2.09354114, Discriminator loss: 0.64752829\n",
      "Epoch 67 of 300\n",
      "Generator loss: 2.04678067, Discriminator loss: 0.68758591\n",
      "Epoch 68 of 300\n",
      "Generator loss: 1.99578074, Discriminator loss: 0.68615655\n",
      "Epoch 69 of 300\n",
      "Generator loss: 1.94894976, Discriminator loss: 0.70908585\n",
      "Epoch 70 of 300\n",
      "Generator loss: 1.96364013, Discriminator loss: 0.71646176\n",
      "Epoch 71 of 300\n",
      "Generator loss: 1.94003283, Discriminator loss: 0.72749816\n",
      "Epoch 72 of 300\n",
      "Generator loss: 1.82624633, Discriminator loss: 0.77770618\n",
      "Epoch 73 of 300\n",
      "Generator loss: 1.87715111, Discriminator loss: 0.76324257\n",
      "Epoch 74 of 300\n",
      "Generator loss: 1.75145628, Discriminator loss: 0.78117642\n",
      "Epoch 75 of 300\n",
      "Generator loss: 1.81363489, Discriminator loss: 0.78506808\n",
      "Epoch 76 of 300\n",
      "Generator loss: 1.86968660, Discriminator loss: 0.77318993\n",
      "Epoch 77 of 300\n",
      "Generator loss: 1.79719760, Discriminator loss: 0.79018326\n",
      "Epoch 78 of 300\n",
      "Generator loss: 1.72395663, Discriminator loss: 0.81175301\n",
      "Epoch 79 of 300\n",
      "Generator loss: 1.74440411, Discriminator loss: 0.82065152\n",
      "Epoch 80 of 300\n",
      "Generator loss: 1.63217046, Discriminator loss: 0.84483851\n",
      "Epoch 81 of 300\n",
      "Generator loss: 1.72734887, Discriminator loss: 0.83749966\n",
      "Epoch 82 of 300\n",
      "Generator loss: 1.70329294, Discriminator loss: 0.83348602\n",
      "Epoch 83 of 300\n",
      "Generator loss: 1.61441002, Discriminator loss: 0.87541189\n",
      "Epoch 84 of 300\n",
      "Generator loss: 1.54025387, Discriminator loss: 0.90138685\n",
      "Epoch 85 of 300\n",
      "Generator loss: 1.53302277, Discriminator loss: 0.90718542\n",
      "Epoch 86 of 300\n",
      "Generator loss: 1.58336390, Discriminator loss: 0.89766936\n",
      "Epoch 87 of 300\n",
      "Generator loss: 1.57118824, Discriminator loss: 0.89183662\n",
      "Epoch 88 of 300\n",
      "Generator loss: 1.46623618, Discriminator loss: 0.94608355\n",
      "Epoch 89 of 300\n",
      "Generator loss: 1.55050762, Discriminator loss: 0.91883504\n",
      "Epoch 90 of 300\n",
      "Generator loss: 1.57468583, Discriminator loss: 0.90498867\n",
      "Epoch 91 of 300\n",
      "Generator loss: 1.52956831, Discriminator loss: 0.92956499\n",
      "Epoch 92 of 300\n",
      "Generator loss: 1.45604793, Discriminator loss: 0.94757661\n",
      "Epoch 93 of 300\n",
      "Generator loss: 1.43293199, Discriminator loss: 0.95319660\n",
      "Epoch 94 of 300\n",
      "Generator loss: 1.41009265, Discriminator loss: 0.96901515\n",
      "Epoch 95 of 300\n",
      "Generator loss: 1.43975532, Discriminator loss: 0.96192888\n",
      "Epoch 96 of 300\n",
      "Generator loss: 1.44593845, Discriminator loss: 0.95472410\n",
      "Epoch 97 of 300\n",
      "Generator loss: 1.40251526, Discriminator loss: 0.96699350\n",
      "Epoch 98 of 300\n",
      "Generator loss: 1.38206799, Discriminator loss: 0.98533093\n",
      "Epoch 99 of 300\n",
      "Generator loss: 1.37709276, Discriminator loss: 0.98428171\n",
      "Epoch 100 of 300\n",
      "Generator loss: 1.34792432, Discriminator loss: 0.99766139\n",
      "Epoch 101 of 300\n",
      "Generator loss: 1.29046400, Discriminator loss: 1.02773932\n",
      "Epoch 102 of 300\n",
      "Generator loss: 1.30282773, Discriminator loss: 1.02283108\n",
      "Epoch 103 of 300\n",
      "Generator loss: 1.29738541, Discriminator loss: 1.03084573\n",
      "Epoch 104 of 300\n",
      "Generator loss: 1.32398418, Discriminator loss: 1.02445643\n",
      "Epoch 105 of 300\n",
      "Generator loss: 1.25475317, Discriminator loss: 1.04768236\n",
      "Epoch 106 of 300\n",
      "Generator loss: 1.28401558, Discriminator loss: 1.05019649\n",
      "Epoch 107 of 300\n",
      "Generator loss: 1.27605695, Discriminator loss: 1.04581693\n",
      "Epoch 108 of 300\n",
      "Generator loss: 1.27933341, Discriminator loss: 1.04625897\n",
      "Epoch 109 of 300\n",
      "Generator loss: 1.28221075, Discriminator loss: 1.04870898\n",
      "Epoch 110 of 300\n",
      "Generator loss: 1.27635816, Discriminator loss: 1.04880033\n",
      "Epoch 111 of 300\n",
      "Generator loss: 1.25637136, Discriminator loss: 1.05881116\n",
      "Epoch 112 of 300\n",
      "Generator loss: 1.27077066, Discriminator loss: 1.05221063\n",
      "Epoch 113 of 300\n",
      "Generator loss: 1.24121141, Discriminator loss: 1.07146572\n",
      "Epoch 114 of 300\n",
      "Generator loss: 1.21802913, Discriminator loss: 1.06015553\n",
      "Epoch 115 of 300\n",
      "Generator loss: 1.22994115, Discriminator loss: 1.06760141\n",
      "Epoch 116 of 300\n",
      "Generator loss: 1.19637988, Discriminator loss: 1.09307750\n",
      "Epoch 117 of 300\n",
      "Generator loss: 1.21353955, Discriminator loss: 1.09328309\n",
      "Epoch 118 of 300\n",
      "Generator loss: 1.16397005, Discriminator loss: 1.09998572\n",
      "Epoch 119 of 300\n",
      "Generator loss: 1.19287252, Discriminator loss: 1.09830690\n",
      "Epoch 120 of 300\n",
      "Generator loss: 1.16265483, Discriminator loss: 1.10668881\n",
      "Epoch 121 of 300\n",
      "Generator loss: 1.14065341, Discriminator loss: 1.11638598\n",
      "Epoch 122 of 300\n",
      "Generator loss: 1.18834707, Discriminator loss: 1.10955563\n",
      "Epoch 123 of 300\n",
      "Generator loss: 1.15586730, Discriminator loss: 1.11369037\n",
      "Epoch 124 of 300\n",
      "Generator loss: 1.17280659, Discriminator loss: 1.11449606\n",
      "Epoch 125 of 300\n",
      "Generator loss: 1.15785394, Discriminator loss: 1.11790680\n",
      "Epoch 126 of 300\n",
      "Generator loss: 1.16329255, Discriminator loss: 1.11708594\n",
      "Epoch 127 of 300\n",
      "Generator loss: 1.15788310, Discriminator loss: 1.12695347\n",
      "Epoch 128 of 300\n",
      "Generator loss: 1.12991236, Discriminator loss: 1.12903960\n",
      "Epoch 129 of 300\n",
      "Generator loss: 1.11876388, Discriminator loss: 1.13848307\n",
      "Epoch 130 of 300\n",
      "Generator loss: 1.14760232, Discriminator loss: 1.12719925\n",
      "Epoch 131 of 300\n",
      "Generator loss: 1.10898191, Discriminator loss: 1.13859521\n",
      "Epoch 132 of 300\n",
      "Generator loss: 1.09002932, Discriminator loss: 1.15255404\n",
      "Epoch 133 of 300\n",
      "Generator loss: 1.08067908, Discriminator loss: 1.15676771\n",
      "Epoch 134 of 300\n",
      "Generator loss: 1.06210349, Discriminator loss: 1.16499381\n",
      "Epoch 135 of 300\n",
      "Generator loss: 1.07104274, Discriminator loss: 1.15913810\n",
      "Epoch 136 of 300\n",
      "Generator loss: 1.07368415, Discriminator loss: 1.17576870\n",
      "Epoch 137 of 300\n",
      "Generator loss: 1.06279919, Discriminator loss: 1.16691797\n",
      "Epoch 138 of 300\n",
      "Generator loss: 1.09262511, Discriminator loss: 1.16059950\n",
      "Epoch 139 of 300\n",
      "Generator loss: 1.06777708, Discriminator loss: 1.16807790\n",
      "Epoch 140 of 300\n",
      "Generator loss: 1.05210653, Discriminator loss: 1.17974221\n",
      "Epoch 141 of 300\n",
      "Generator loss: 1.04654650, Discriminator loss: 1.18241598\n",
      "Epoch 142 of 300\n",
      "Generator loss: 1.01893978, Discriminator loss: 1.18948862\n",
      "Epoch 143 of 300\n",
      "Generator loss: 1.03505878, Discriminator loss: 1.18755677\n",
      "Epoch 144 of 300\n",
      "Generator loss: 1.04581748, Discriminator loss: 1.18776611\n",
      "Epoch 145 of 300\n",
      "Generator loss: 1.04905173, Discriminator loss: 1.17523012\n",
      "Epoch 146 of 300\n",
      "Generator loss: 1.04453953, Discriminator loss: 1.18074219\n",
      "Epoch 147 of 300\n",
      "Generator loss: 1.06019625, Discriminator loss: 1.17532265\n",
      "Epoch 148 of 300\n",
      "Generator loss: 1.07352302, Discriminator loss: 1.17138818\n",
      "Epoch 149 of 300\n",
      "Generator loss: 1.05759935, Discriminator loss: 1.17411722\n",
      "Epoch 150 of 300\n",
      "Generator loss: 1.03758510, Discriminator loss: 1.18526059\n",
      "Epoch 151 of 300\n",
      "Generator loss: 1.01394213, Discriminator loss: 1.20311643\n",
      "Epoch 152 of 300\n",
      "Generator loss: 1.02717926, Discriminator loss: 1.19867013\n",
      "Epoch 153 of 300\n",
      "Generator loss: 1.02182276, Discriminator loss: 1.19849317\n",
      "Epoch 154 of 300\n",
      "Generator loss: 1.05321596, Discriminator loss: 1.18485775\n",
      "Epoch 155 of 300\n",
      "Generator loss: 1.09246109, Discriminator loss: 1.16972590\n",
      "Epoch 156 of 300\n",
      "Generator loss: 1.09536262, Discriminator loss: 1.17414893\n",
      "Epoch 157 of 300\n",
      "Generator loss: 1.04923326, Discriminator loss: 1.18773134\n",
      "Epoch 158 of 300\n",
      "Generator loss: 1.02040730, Discriminator loss: 1.19577673\n",
      "Epoch 159 of 300\n",
      "Generator loss: 1.03091110, Discriminator loss: 1.19760657\n",
      "Epoch 160 of 300\n",
      "Generator loss: 1.03878852, Discriminator loss: 1.19053312\n",
      "Epoch 161 of 300\n",
      "Generator loss: 0.98596072, Discriminator loss: 1.21410102\n",
      "Epoch 162 of 300\n",
      "Generator loss: 0.98274628, Discriminator loss: 1.21863752\n",
      "Epoch 163 of 300\n",
      "Generator loss: 0.99253149, Discriminator loss: 1.20984979\n",
      "Epoch 164 of 300\n",
      "Generator loss: 0.98409441, Discriminator loss: 1.21658536\n",
      "Epoch 165 of 300\n",
      "Generator loss: 1.01166576, Discriminator loss: 1.20433601\n",
      "Epoch 166 of 300\n",
      "Generator loss: 0.96232476, Discriminator loss: 1.22437917\n",
      "Epoch 167 of 300\n",
      "Generator loss: 0.96316474, Discriminator loss: 1.22713435\n",
      "Epoch 168 of 300\n",
      "Generator loss: 0.98707800, Discriminator loss: 1.21676018\n",
      "Epoch 169 of 300\n",
      "Generator loss: 0.95224550, Discriminator loss: 1.23845870\n",
      "Epoch 170 of 300\n",
      "Generator loss: 0.95696637, Discriminator loss: 1.23766974\n",
      "Epoch 171 of 300\n",
      "Generator loss: 0.94579072, Discriminator loss: 1.23982865\n",
      "Epoch 172 of 300\n",
      "Generator loss: 0.97857059, Discriminator loss: 1.23120613\n",
      "Epoch 173 of 300\n",
      "Generator loss: 0.97500409, Discriminator loss: 1.22984359\n",
      "Epoch 174 of 300\n",
      "Generator loss: 0.99458980, Discriminator loss: 1.21938039\n",
      "Epoch 175 of 300\n",
      "Generator loss: 0.97396880, Discriminator loss: 1.22848239\n",
      "Epoch 176 of 300\n",
      "Generator loss: 0.97429122, Discriminator loss: 1.22954907\n",
      "Epoch 177 of 300\n",
      "Generator loss: 0.97110443, Discriminator loss: 1.22629697\n",
      "Epoch 178 of 300\n",
      "Generator loss: 0.94722829, Discriminator loss: 1.24068124\n",
      "Epoch 179 of 300\n",
      "Generator loss: 0.94945941, Discriminator loss: 1.23858747\n",
      "Epoch 180 of 300\n",
      "Generator loss: 0.96150659, Discriminator loss: 1.23756237\n",
      "Epoch 181 of 300\n",
      "Generator loss: 0.99366359, Discriminator loss: 1.21598746\n",
      "Epoch 182 of 300\n",
      "Generator loss: 0.97048443, Discriminator loss: 1.23354526\n",
      "Epoch 183 of 300\n",
      "Generator loss: 0.95541151, Discriminator loss: 1.24084924\n",
      "Epoch 184 of 300\n",
      "Generator loss: 0.98203928, Discriminator loss: 1.22706894\n",
      "Epoch 185 of 300\n",
      "Generator loss: 0.95545296, Discriminator loss: 1.23897083\n",
      "Epoch 186 of 300\n",
      "Generator loss: 0.93155404, Discriminator loss: 1.23951135\n",
      "Epoch 187 of 300\n",
      "Generator loss: 0.92999942, Discriminator loss: 1.25358091\n",
      "Epoch 188 of 300\n",
      "Generator loss: 0.91869438, Discriminator loss: 1.25761688\n",
      "Epoch 189 of 300\n",
      "Generator loss: 0.94419443, Discriminator loss: 1.24552755\n",
      "Epoch 190 of 300\n",
      "Generator loss: 0.95096760, Discriminator loss: 1.23746447\n",
      "Epoch 191 of 300\n",
      "Generator loss: 0.94285401, Discriminator loss: 1.24620424\n",
      "Epoch 192 of 300\n",
      "Generator loss: 0.93182902, Discriminator loss: 1.24719468\n",
      "Epoch 193 of 300\n",
      "Generator loss: 0.91426098, Discriminator loss: 1.25608901\n",
      "Epoch 194 of 300\n",
      "Generator loss: 0.92472616, Discriminator loss: 1.25654562\n",
      "Epoch 195 of 300\n",
      "Generator loss: 0.92032674, Discriminator loss: 1.25451666\n",
      "Epoch 196 of 300\n",
      "Generator loss: 0.92220084, Discriminator loss: 1.25406009\n",
      "Epoch 197 of 300\n",
      "Generator loss: 0.94208534, Discriminator loss: 1.25235082\n",
      "Epoch 198 of 300\n",
      "Generator loss: 0.93517343, Discriminator loss: 1.25154874\n",
      "Epoch 199 of 300\n",
      "Generator loss: 0.92338083, Discriminator loss: 1.25461522\n",
      "Epoch 200 of 300\n",
      "Generator loss: 0.91553499, Discriminator loss: 1.25765376\n",
      "Epoch 201 of 300\n",
      "Generator loss: 0.93639749, Discriminator loss: 1.25314743\n",
      "Epoch 202 of 300\n",
      "Generator loss: 0.92197998, Discriminator loss: 1.26296675\n",
      "Epoch 203 of 300\n",
      "Generator loss: 0.92730042, Discriminator loss: 1.25717227\n",
      "Epoch 204 of 300\n",
      "Generator loss: 0.90460400, Discriminator loss: 1.26535313\n",
      "Epoch 205 of 300\n",
      "Generator loss: 0.90992064, Discriminator loss: 1.25834712\n",
      "Epoch 206 of 300\n",
      "Generator loss: 0.93768077, Discriminator loss: 1.25317958\n",
      "Epoch 207 of 300\n",
      "Generator loss: 0.92131208, Discriminator loss: 1.25964123\n",
      "Epoch 208 of 300\n",
      "Generator loss: 0.94493068, Discriminator loss: 1.24826170\n",
      "Epoch 209 of 300\n",
      "Generator loss: 0.92596314, Discriminator loss: 1.25604910\n",
      "Epoch 210 of 300\n",
      "Generator loss: 0.91924129, Discriminator loss: 1.26266676\n",
      "Epoch 211 of 300\n",
      "Generator loss: 0.91633258, Discriminator loss: 1.26629682\n",
      "Epoch 212 of 300\n",
      "Generator loss: 0.90298323, Discriminator loss: 1.26882761\n",
      "Epoch 213 of 300\n",
      "Generator loss: 0.90848111, Discriminator loss: 1.26865935\n",
      "Epoch 214 of 300\n",
      "Generator loss: 0.89757910, Discriminator loss: 1.26740434\n",
      "Epoch 215 of 300\n",
      "Generator loss: 0.89144609, Discriminator loss: 1.27856864\n",
      "Epoch 216 of 300\n",
      "Generator loss: 0.89880692, Discriminator loss: 1.27015428\n",
      "Epoch 217 of 300\n",
      "Generator loss: 0.90872891, Discriminator loss: 1.26783762\n",
      "Epoch 218 of 300\n",
      "Generator loss: 0.89909982, Discriminator loss: 1.27133763\n",
      "Epoch 219 of 300\n",
      "Generator loss: 0.90477342, Discriminator loss: 1.27075569\n",
      "Epoch 220 of 300\n",
      "Generator loss: 0.90342436, Discriminator loss: 1.26505315\n",
      "Epoch 221 of 300\n",
      "Generator loss: 0.90356881, Discriminator loss: 1.27100696\n",
      "Epoch 222 of 300\n",
      "Generator loss: 0.90097450, Discriminator loss: 1.27140942\n",
      "Epoch 223 of 300\n",
      "Generator loss: 0.90943426, Discriminator loss: 1.26825465\n",
      "Epoch 224 of 300\n",
      "Generator loss: 0.89269645, Discriminator loss: 1.27302656\n",
      "Epoch 225 of 300\n",
      "Generator loss: 0.90727769, Discriminator loss: 1.26747259\n",
      "Epoch 226 of 300\n",
      "Generator loss: 0.89207220, Discriminator loss: 1.27409647\n",
      "Epoch 227 of 300\n",
      "Generator loss: 0.88052230, Discriminator loss: 1.27450329\n",
      "Epoch 228 of 300\n",
      "Generator loss: 0.89719242, Discriminator loss: 1.26389241\n",
      "Epoch 229 of 300\n",
      "Generator loss: 0.90184309, Discriminator loss: 1.27689846\n",
      "Epoch 230 of 300\n",
      "Generator loss: 0.88369584, Discriminator loss: 1.27996896\n",
      "Epoch 231 of 300\n",
      "Generator loss: 0.88818801, Discriminator loss: 1.27707390\n",
      "Epoch 232 of 300\n",
      "Generator loss: 0.88794483, Discriminator loss: 1.27842875\n",
      "Epoch 233 of 300\n",
      "Generator loss: 0.88787002, Discriminator loss: 1.27465272\n",
      "Epoch 234 of 300\n",
      "Generator loss: 0.87686786, Discriminator loss: 1.28068617\n",
      "Epoch 235 of 300\n",
      "Generator loss: 0.89005599, Discriminator loss: 1.28100060\n",
      "Epoch 236 of 300\n",
      "Generator loss: 0.88456277, Discriminator loss: 1.28012341\n",
      "Epoch 237 of 300\n",
      "Generator loss: 0.89791415, Discriminator loss: 1.27269232\n",
      "Epoch 238 of 300\n",
      "Generator loss: 0.89429895, Discriminator loss: 1.27801552\n",
      "Epoch 239 of 300\n",
      "Generator loss: 0.90405867, Discriminator loss: 1.27392759\n",
      "Epoch 240 of 300\n",
      "Generator loss: 0.88761915, Discriminator loss: 1.27564436\n",
      "Epoch 241 of 300\n",
      "Generator loss: 0.87766602, Discriminator loss: 1.28530911\n",
      "Epoch 242 of 300\n",
      "Generator loss: 0.87374152, Discriminator loss: 1.28581246\n",
      "Epoch 243 of 300\n",
      "Generator loss: 0.87731095, Discriminator loss: 1.28276980\n",
      "Epoch 244 of 300\n",
      "Generator loss: 0.86483748, Discriminator loss: 1.28789274\n",
      "Epoch 245 of 300\n",
      "Generator loss: 0.86075724, Discriminator loss: 1.29033576\n",
      "Epoch 246 of 300\n",
      "Generator loss: 0.86025713, Discriminator loss: 1.29341584\n",
      "Epoch 247 of 300\n",
      "Generator loss: 0.85883983, Discriminator loss: 1.29327054\n",
      "Epoch 248 of 300\n",
      "Generator loss: 0.88354634, Discriminator loss: 1.28299515\n",
      "Epoch 249 of 300\n",
      "Generator loss: 0.87643460, Discriminator loss: 1.29369676\n",
      "Epoch 250 of 300\n",
      "Generator loss: 0.88221375, Discriminator loss: 1.28063865\n",
      "Epoch 251 of 300\n",
      "Generator loss: 0.86630427, Discriminator loss: 1.28622562\n",
      "Epoch 252 of 300\n",
      "Generator loss: 0.87676087, Discriminator loss: 1.28536640\n",
      "Epoch 253 of 300\n",
      "Generator loss: 0.87910608, Discriminator loss: 1.28438903\n",
      "Epoch 254 of 300\n",
      "Generator loss: 0.89167177, Discriminator loss: 1.28152806\n",
      "Epoch 255 of 300\n",
      "Generator loss: 0.89542599, Discriminator loss: 1.27399695\n",
      "Epoch 256 of 300\n",
      "Generator loss: 0.84928289, Discriminator loss: 1.29818501\n",
      "Epoch 257 of 300\n",
      "Generator loss: 0.88533104, Discriminator loss: 1.28480517\n",
      "Epoch 258 of 300\n",
      "Generator loss: 0.85229208, Discriminator loss: 1.29539344\n",
      "Epoch 259 of 300\n",
      "Generator loss: 0.85812979, Discriminator loss: 1.29441078\n",
      "Epoch 260 of 300\n",
      "Generator loss: 0.88747907, Discriminator loss: 1.28080180\n",
      "Epoch 261 of 300\n",
      "Generator loss: 0.86191299, Discriminator loss: 1.29248554\n",
      "Epoch 262 of 300\n",
      "Generator loss: 0.85557814, Discriminator loss: 1.29370184\n",
      "Epoch 263 of 300\n",
      "Generator loss: 0.86459580, Discriminator loss: 1.28929680\n",
      "Epoch 264 of 300\n",
      "Generator loss: 0.88809736, Discriminator loss: 1.28633484\n",
      "Epoch 265 of 300\n",
      "Generator loss: 0.87611604, Discriminator loss: 1.28163584\n",
      "Epoch 266 of 300\n",
      "Generator loss: 0.85788776, Discriminator loss: 1.29444022\n",
      "Epoch 267 of 300\n",
      "Generator loss: 0.87004214, Discriminator loss: 1.29059630\n",
      "Epoch 268 of 300\n",
      "Generator loss: 0.84827166, Discriminator loss: 1.29633724\n",
      "Epoch 269 of 300\n",
      "Generator loss: 0.85945138, Discriminator loss: 1.29535672\n",
      "Epoch 270 of 300\n",
      "Generator loss: 0.86366961, Discriminator loss: 1.29391045\n",
      "Epoch 271 of 300\n",
      "Generator loss: 0.86721319, Discriminator loss: 1.29132503\n",
      "Epoch 272 of 300\n",
      "Generator loss: 0.86463742, Discriminator loss: 1.29404963\n",
      "Epoch 273 of 300\n",
      "Generator loss: 0.85443128, Discriminator loss: 1.29448362\n",
      "Epoch 274 of 300\n",
      "Generator loss: 0.83866845, Discriminator loss: 1.29988775\n",
      "Epoch 275 of 300\n",
      "Generator loss: 0.86097080, Discriminator loss: 1.29716083\n",
      "Epoch 276 of 300\n",
      "Generator loss: 0.87517462, Discriminator loss: 1.29510418\n",
      "Epoch 277 of 300\n",
      "Generator loss: 0.85639278, Discriminator loss: 1.29486877\n",
      "Epoch 278 of 300\n",
      "Generator loss: 0.87358402, Discriminator loss: 1.28857449\n",
      "Epoch 279 of 300\n",
      "Generator loss: 0.85361769, Discriminator loss: 1.29800225\n",
      "Epoch 280 of 300\n",
      "Generator loss: 0.86446027, Discriminator loss: 1.29080758\n",
      "Epoch 281 of 300\n",
      "Generator loss: 0.86327431, Discriminator loss: 1.29177651\n",
      "Epoch 282 of 300\n",
      "Generator loss: 0.85248711, Discriminator loss: 1.30071035\n",
      "Epoch 283 of 300\n",
      "Generator loss: 0.86032974, Discriminator loss: 1.29280776\n",
      "Epoch 284 of 300\n",
      "Generator loss: 0.86905930, Discriminator loss: 1.28606003\n",
      "Epoch 285 of 300\n",
      "Generator loss: 0.83896311, Discriminator loss: 1.31079713\n",
      "Epoch 286 of 300\n",
      "Generator loss: 0.84226251, Discriminator loss: 1.30142945\n",
      "Epoch 287 of 300\n",
      "Generator loss: 0.85254631, Discriminator loss: 1.29671847\n",
      "Epoch 288 of 300\n",
      "Generator loss: 0.86562356, Discriminator loss: 1.29361396\n",
      "Epoch 289 of 300\n",
      "Generator loss: 0.86494511, Discriminator loss: 1.28843550\n",
      "Epoch 290 of 300\n",
      "Generator loss: 0.85270245, Discriminator loss: 1.29754956\n",
      "Epoch 291 of 300\n",
      "Generator loss: 0.85929596, Discriminator loss: 1.29722229\n",
      "Epoch 292 of 300\n",
      "Generator loss: 0.86739310, Discriminator loss: 1.29046165\n",
      "Epoch 293 of 300\n",
      "Generator loss: 0.87085652, Discriminator loss: 1.29466153\n",
      "Epoch 294 of 300\n",
      "Generator loss: 0.83939838, Discriminator loss: 1.30688122\n",
      "Epoch 295 of 300\n",
      "Generator loss: 0.84216412, Discriminator loss: 1.30857060\n",
      "Epoch 296 of 300\n",
      "Generator loss: 0.84843268, Discriminator loss: 1.30252046\n",
      "Epoch 297 of 300\n",
      "Generator loss: 0.83973556, Discriminator loss: 1.30081985\n",
      "Epoch 298 of 300\n",
      "Generator loss: 0.84285417, Discriminator loss: 1.30228384\n",
      "Epoch 299 of 300\n",
      "Generator loss: 0.86562693, Discriminator loss: 1.29440606\n",
      "DONE TRAINING\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"249.421034pt\" version=\"1.1\" viewBox=\"0 0 362.5625 249.421034\" width=\"362.5625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 249.421034 \nL 362.5625 249.421034 \nL 362.5625 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 225.542909 \nL 355.3625 225.542909 \nL 355.3625 8.102909 \nL 20.5625 8.102909 \nz\n\" style=\"fill:#e5e5e5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 35.780682 225.542909 \nL 35.780682 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m93ecf79b9f\" style=\"stroke:#555555;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"35.780682\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(32.599432 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 86.677611 225.542909 \nL 86.677611 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"86.677611\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(80.315111 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 137.57454 225.542909 \nL 137.57454 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"137.57454\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(128.03079 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 188.471469 225.542909 \nL 188.471469 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"188.471469\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g style=\"fill:#555555;\" transform=\"translate(178.927719 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 239.368398 225.542909 \nL 239.368398 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"239.368398\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(229.824648 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 290.265328 225.542909 \nL 290.265328 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"290.265328\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g style=\"fill:#555555;\" transform=\"translate(280.721578 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 341.162257 225.542909 \nL 341.162257 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"341.162257\" xlink:href=\"#m93ecf79b9f\" y=\"225.542909\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(331.618507 240.141346)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 217.013483 \nL 355.3625 217.013483 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m49e9c4d915\" style=\"stroke:#555555;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"217.013483\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 220.812701)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 191.2617 \nL 355.3625 191.2617 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"191.2617\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 195.060918)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 165.509917 \nL 355.3625 165.509917 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"165.509917\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 169.309135)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 139.758134 \nL 355.3625 139.758134 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"139.758134\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 143.557352)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 114.006351 \nL 355.3625 114.006351 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"114.006351\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 117.805569)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 88.254568 \nL 355.3625 88.254568 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"88.254568\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 5 -->\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 92.053786)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 62.502785 \nL 355.3625 62.502785 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"62.502785\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 66.302003)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 36.751002 \nL 355.3625 36.751002 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"36.751002\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 40.55022)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#pae7947eae5)\" d=\"M 20.5625 10.999219 \nL 355.3625 10.999219 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"fill:#555555;stroke:#555555;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m49e9c4d915\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#555555;\" transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#pae7947eae5)\" d=\"M 35.780682 176.148008 \nL 36.79862 123.833248 \nL 37.816559 36.373673 \nL 38.834498 78.370829 \nL 39.852436 78.460564 \nL 40.870375 83.06983 \nL 41.888313 67.093514 \nL 42.906252 70.968576 \nL 43.92419 61.640227 \nL 44.942129 57.503515 \nL 45.960068 55.772203 \nL 46.978006 63.125231 \nL 47.995945 58.634199 \nL 49.013883 17.986545 \nL 50.031822 70.518502 \nL 51.049761 66.568022 \nL 52.067699 70.929928 \nL 53.085638 71.782263 \nL 54.103576 84.001579 \nL 55.121515 72.90637 \nL 56.139453 92.568678 \nL 57.157392 86.865122 \nL 58.175331 97.676125 \nL 59.193269 84.744309 \nL 60.211208 98.254528 \nL 61.229146 100.194922 \nL 62.247085 94.016239 \nL 63.265024 93.913987 \nL 64.282962 96.734773 \nL 65.300901 94.188481 \nL 66.318839 93.327661 \nL 68.354716 117.017311 \nL 69.372655 107.62002 \nL 70.390594 121.988686 \nL 71.408532 120.820806 \nL 72.426471 117.648125 \nL 73.444409 126.906131 \nL 74.462348 125.107795 \nL 75.480287 129.072852 \nL 76.498225 132.452412 \nL 77.516164 130.792142 \nL 78.534102 135.91042 \nL 80.569979 140.828725 \nL 81.587918 138.739629 \nL 82.605857 140.514236 \nL 83.623795 138.243918 \nL 85.659672 146.191672 \nL 86.677611 146.734517 \nL 87.69555 148.152143 \nL 88.713488 151.035252 \nL 89.731427 153.574104 \nL 90.749365 152.350564 \nL 91.767304 156.332497 \nL 92.785242 155.169835 \nL 93.803181 158.964969 \nL 94.82112 160.126308 \nL 95.839058 156.911903 \nL 96.856997 161.53999 \nL 97.874935 162.251708 \nL 98.892874 162.107566 \nL 99.910813 159.742241 \nL 100.928751 162.910279 \nL 101.94669 163.746899 \nL 102.964628 163.101066 \nL 106.018444 166.824551 \nL 107.036383 166.446248 \nL 108.054321 167.054178 \nL 109.07226 169.984383 \nL 110.090198 168.673495 \nL 111.108137 171.910361 \nL 113.144014 168.865719 \nL 115.179891 172.618526 \nL 116.19783 172.091967 \nL 117.215768 174.982183 \nL 118.233707 172.531169 \nL 119.251646 173.150653 \nL 120.269584 175.439546 \nL 121.287523 177.349199 \nL 122.305461 177.535413 \nL 123.3234 176.239039 \nL 124.341339 176.552584 \nL 125.359277 179.255287 \nL 126.377216 177.085147 \nL 127.395154 176.462515 \nL 128.413093 177.624371 \nL 129.431031 179.517652 \nL 131.466909 180.701083 \nL 132.484847 179.937216 \nL 133.502786 179.777989 \nL 134.520724 180.896214 \nL 135.538663 181.422768 \nL 136.556602 181.550889 \nL 137.57454 182.302028 \nL 138.592479 183.781734 \nL 139.610417 183.463346 \nL 140.628356 183.603495 \nL 141.646294 182.918529 \nL 142.664233 184.701351 \nL 143.682172 183.947792 \nL 144.70011 184.152741 \nL 146.735987 183.99427 \nL 147.753926 184.144984 \nL 148.771865 184.65968 \nL 149.789803 184.288872 \nL 150.807742 185.050076 \nL 151.82568 185.647061 \nL 152.843619 185.340305 \nL 153.861557 186.204568 \nL 154.879496 185.762676 \nL 155.897435 187.039179 \nL 156.915373 186.294888 \nL 157.933312 187.073048 \nL 158.95125 187.639623 \nL 159.969189 186.411427 \nL 160.987128 187.247839 \nL 162.005066 186.811622 \nL 163.023005 187.196679 \nL 164.040943 187.056625 \nL 165.058882 187.195928 \nL 166.07682 187.916225 \nL 167.094759 188.203318 \nL 168.112698 187.460677 \nL 169.130636 188.455221 \nL 170.148575 188.943284 \nL 171.166513 189.184069 \nL 172.184452 189.662424 \nL 173.202391 189.432222 \nL 174.220329 189.364201 \nL 175.238268 189.644509 \nL 176.256206 188.876438 \nL 177.274145 189.516319 \nL 178.292083 189.919864 \nL 179.310022 190.063044 \nL 180.327961 190.773967 \nL 181.345899 190.358874 \nL 182.363838 190.081818 \nL 183.381776 189.99853 \nL 184.399715 190.114727 \nL 186.435592 189.368351 \nL 188.471469 190.293816 \nL 189.489408 190.902665 \nL 190.507346 190.561785 \nL 191.525285 190.699725 \nL 192.543224 189.891294 \nL 193.561162 188.880662 \nL 194.579101 188.805942 \nL 195.597039 189.993855 \nL 196.614978 190.736175 \nL 198.650855 190.262826 \nL 199.668794 191.623236 \nL 200.686732 191.706014 \nL 201.704671 191.454027 \nL 202.722609 191.671297 \nL 203.740548 190.961285 \nL 204.758487 192.231904 \nL 205.776425 192.210273 \nL 206.794364 191.594464 \nL 207.812302 192.491463 \nL 208.830241 192.369892 \nL 209.84818 192.657685 \nL 210.866118 191.813545 \nL 211.884057 191.905389 \nL 212.901995 191.401022 \nL 213.919934 191.932049 \nL 215.955811 192.005812 \nL 216.97375 192.620665 \nL 217.991688 192.56321 \nL 219.009627 192.252974 \nL 220.027565 191.424873 \nL 221.045504 192.021778 \nL 222.063443 192.409933 \nL 223.081381 191.72422 \nL 225.117258 193.024305 \nL 226.135197 193.064339 \nL 227.153135 193.355464 \nL 228.171074 192.698792 \nL 229.189013 192.524371 \nL 231.22489 193.017224 \nL 232.242828 193.469632 \nL 233.260767 193.200135 \nL 234.278706 193.313428 \nL 235.296644 193.265167 \nL 236.314583 192.753105 \nL 237.332521 192.931099 \nL 239.368398 193.436824 \nL 240.386337 192.899578 \nL 241.404276 193.270854 \nL 242.422214 193.133843 \nL 243.440153 193.718317 \nL 244.458091 193.581404 \nL 245.47603 192.866531 \nL 246.493969 193.288054 \nL 247.511907 192.679833 \nL 248.529846 193.168281 \nL 251.583661 193.760054 \nL 252.6016 193.618474 \nL 253.619539 193.89922 \nL 254.637477 194.057156 \nL 256.673354 193.612093 \nL 257.691293 193.860059 \nL 258.709232 193.713954 \nL 261.763047 193.811783 \nL 262.780986 193.593929 \nL 263.798924 194.024957 \nL 264.816863 193.649464 \nL 266.85274 194.338463 \nL 267.870679 193.909178 \nL 268.888617 193.789415 \nL 269.906556 194.256739 \nL 270.924495 194.141058 \nL 272.960372 194.149246 \nL 273.97831 194.432572 \nL 274.996249 194.092954 \nL 276.014187 194.234414 \nL 277.032126 193.890592 \nL 278.050065 193.98369 \nL 279.068003 193.73236 \nL 280.085942 194.155707 \nL 281.10388 194.412018 \nL 282.121819 194.513081 \nL 283.139758 194.421161 \nL 284.157696 194.742376 \nL 286.193573 194.860328 \nL 287.211512 194.896826 \nL 288.22945 194.260589 \nL 289.247389 194.443729 \nL 290.265328 194.294906 \nL 291.283266 194.704603 \nL 292.301205 194.435327 \nL 293.319143 194.374934 \nL 294.337082 194.051345 \nL 295.355021 193.954667 \nL 296.372959 195.142934 \nL 297.390898 194.21463 \nL 298.408836 195.065442 \nL 299.426775 194.91511 \nL 300.444713 194.159314 \nL 301.462652 194.817686 \nL 302.480591 194.98082 \nL 303.498529 194.748599 \nL 304.516468 194.143392 \nL 305.534406 194.451932 \nL 306.552345 194.921343 \nL 307.570284 194.608346 \nL 308.588222 195.168975 \nL 309.606161 194.881077 \nL 311.642038 194.681197 \nL 312.659976 194.747527 \nL 313.677915 195.010354 \nL 314.695854 195.416275 \nL 315.713792 194.841949 \nL 316.731731 194.476176 \nL 317.749669 194.959842 \nL 318.767608 194.517136 \nL 319.785547 195.031305 \nL 320.803485 194.752089 \nL 321.821424 194.78263 \nL 322.839362 195.06042 \nL 324.875239 194.633656 \nL 325.893178 195.408687 \nL 326.911117 195.323721 \nL 328.946994 194.722132 \nL 329.964932 194.739604 \nL 330.982871 195.054874 \nL 334.036687 194.587374 \nL 335.054625 195.397478 \nL 337.090502 195.164828 \nL 338.108441 195.388795 \nL 339.12638 195.308485 \nL 340.144318 194.722046 \nL 340.144318 194.722046 \n\" style=\"fill:none;stroke:#e24a33;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path clip-path=\"url(#pae7947eae5)\" d=\"M 35.780682 200.168539 \nL 36.79862 213.945081 \nL 37.816559 209.208054 \nL 38.834498 210.775567 \nL 39.852436 213.573773 \nL 40.870375 214.29326 \nL 41.888313 215.659272 \nL 42.906252 214.9897 \nL 45.960068 215.600596 \nL 46.978006 215.470595 \nL 47.995945 214.474272 \nL 49.013883 215.217839 \nL 50.031822 214.187862 \nL 51.049761 214.040995 \nL 52.067699 214.855135 \nL 54.103576 214.797858 \nL 55.121515 214.329436 \nL 56.139453 214.065788 \nL 57.157392 213.490388 \nL 58.175331 212.595417 \nL 59.193269 214.479433 \nL 60.211208 212.765662 \nL 61.229146 211.848848 \nL 62.247085 212.196256 \nL 63.265024 212.416372 \nL 64.282962 211.899179 \nL 65.300901 212.140879 \nL 66.318839 212.027852 \nL 67.336778 211.231148 \nL 68.354716 211.543464 \nL 69.372655 209.950234 \nL 70.390594 209.477482 \nL 71.408532 209.499263 \nL 72.426471 209.797774 \nL 73.444409 207.842506 \nL 74.462348 208.626066 \nL 75.480287 206.815797 \nL 76.498225 207.679073 \nL 77.516164 208.118581 \nL 78.534102 207.184294 \nL 79.552041 207.200126 \nL 80.569979 206.502959 \nL 81.587918 206.960419 \nL 82.605857 206.21042 \nL 85.659672 204.888281 \nL 86.677611 204.770664 \nL 87.69555 204.78011 \nL 88.713488 204.06685 \nL 89.731427 202.914355 \nL 90.749365 203.85911 \nL 91.767304 202.056521 \nL 92.785242 202.205277 \nL 93.803181 201.239318 \nL 94.82112 201.013864 \nL 95.839058 201.734675 \nL 96.856997 200.566265 \nL 97.874935 200.508298 \nL 98.892874 200.641735 \nL 99.910813 201.048222 \nL 100.928751 199.922316 \nL 101.94669 200.067316 \nL 102.964628 200.338474 \nL 103.982567 199.306919 \nL 105.000505 199.343728 \nL 106.018444 198.753258 \nL 108.054321 198.279108 \nL 109.07226 196.986162 \nL 110.090198 197.358626 \nL 111.108137 196.896797 \nL 112.126076 196.79658 \nL 113.144014 197.102463 \nL 117.215768 195.257385 \nL 119.251646 195.549732 \nL 120.269584 194.470066 \nL 121.287523 193.801164 \nL 122.305461 193.65184 \nL 124.341339 194.047099 \nL 125.359277 192.650144 \nL 126.377216 193.351842 \nL 127.395154 193.708411 \nL 128.413093 193.075527 \nL 129.431031 192.611695 \nL 130.44897 192.466971 \nL 131.466909 192.059615 \nL 133.502786 192.427635 \nL 134.520724 192.111676 \nL 135.538663 191.639454 \nL 136.556602 191.666473 \nL 137.57454 191.321923 \nL 138.592479 190.547363 \nL 139.610417 190.673759 \nL 140.628356 190.467367 \nL 141.646294 190.631903 \nL 142.664233 190.033794 \nL 143.682172 189.969051 \nL 144.70011 190.081832 \nL 147.753926 190.005004 \nL 148.771865 189.747207 \nL 149.789803 189.917183 \nL 150.807742 189.42133 \nL 151.82568 189.712588 \nL 152.843619 189.520843 \nL 153.861557 188.864788 \nL 154.879496 188.859494 \nL 155.897435 188.686889 \nL 156.915373 188.730122 \nL 158.95125 188.264553 \nL 159.969189 188.440447 \nL 163.023005 188.225389 \nL 164.040943 188.246528 \nL 165.058882 187.992421 \nL 166.07682 187.9387 \nL 167.094759 187.695514 \nL 168.112698 187.986092 \nL 170.148575 187.333161 \nL 172.184452 187.012815 \nL 173.202391 187.16361 \nL 174.220329 186.735342 \nL 176.256206 187.125976 \nL 180.327961 186.38203 \nL 182.363838 186.426387 \nL 183.381776 186.749211 \nL 184.399715 186.607266 \nL 186.435592 186.848148 \nL 187.453531 186.777871 \nL 188.471469 186.490909 \nL 189.489408 186.031089 \nL 191.525285 186.150147 \nL 193.561162 186.890955 \nL 194.579101 186.777054 \nL 195.597039 186.427283 \nL 196.614978 186.2201 \nL 197.632917 186.172978 \nL 198.650855 186.355132 \nL 199.668794 185.748216 \nL 200.686732 185.631394 \nL 201.704671 185.857693 \nL 202.722609 185.68424 \nL 203.740548 185.999683 \nL 204.758487 185.483536 \nL 205.776425 185.412585 \nL 206.794364 185.679738 \nL 207.812302 185.120963 \nL 209.84818 185.085684 \nL 210.866118 185.307729 \nL 211.884057 185.342817 \nL 212.901995 185.612263 \nL 213.919934 185.377871 \nL 214.937872 185.350402 \nL 215.955811 185.434149 \nL 216.97375 185.063728 \nL 219.009627 185.144045 \nL 220.027565 185.699637 \nL 221.045504 185.247493 \nL 222.063443 185.059402 \nL 223.081381 185.41427 \nL 224.09932 185.107775 \nL 225.117258 185.093855 \nL 226.135197 184.731539 \nL 227.153135 184.627606 \nL 229.189013 185.146566 \nL 230.206951 184.921502 \nL 231.22489 184.895996 \nL 232.242828 184.666951 \nL 235.296644 184.719199 \nL 237.332521 184.783871 \nL 239.368398 184.626656 \nL 240.386337 184.742702 \nL 241.404276 184.489837 \nL 242.422214 184.639055 \nL 243.440153 184.428383 \nL 245.47603 184.741874 \nL 246.493969 184.575475 \nL 247.511907 184.868518 \nL 249.547784 184.497562 \nL 251.583661 184.338909 \nL 253.619539 184.375561 \nL 254.637477 184.08806 \nL 255.655416 184.304745 \nL 256.673354 184.364403 \nL 258.709232 184.289258 \nL 259.72717 184.436108 \nL 260.745109 184.282787 \nL 263.798924 184.230779 \nL 264.816863 184.373803 \nL 265.834802 184.203227 \nL 266.85274 184.19275 \nL 267.870679 184.466 \nL 268.888617 184.131071 \nL 269.906556 184.052 \nL 272.960372 184.188902 \nL 273.97831 184.03353 \nL 276.014187 184.048022 \nL 277.032126 184.239386 \nL 278.050065 184.102304 \nL 279.068003 184.207576 \nL 280.085942 184.163366 \nL 281.10388 183.914481 \nL 285.175635 183.785036 \nL 287.211512 183.70946 \nL 288.22945 183.97407 \nL 289.247389 183.698484 \nL 290.265328 184.034754 \nL 291.283266 183.89088 \nL 294.337082 184.01185 \nL 295.355021 184.20579 \nL 296.372959 183.582904 \nL 297.390898 183.927459 \nL 298.408836 183.654792 \nL 299.426775 183.680097 \nL 300.444713 184.030552 \nL 301.462652 183.729675 \nL 302.480591 183.698354 \nL 305.534406 184.009074 \nL 306.552345 183.679339 \nL 307.570284 183.778327 \nL 308.588222 183.630487 \nL 313.677915 183.678221 \nL 314.695854 183.539055 \nL 318.767608 183.830392 \nL 319.785547 183.58761 \nL 320.803485 183.772886 \nL 321.821424 183.747934 \nL 322.839362 183.517872 \nL 324.875239 183.895144 \nL 325.893178 183.258119 \nL 326.911117 183.499354 \nL 329.964932 183.833971 \nL 330.982871 183.599268 \nL 332.00081 183.607696 \nL 333.018748 183.781794 \nL 334.036687 183.67364 \nL 335.054625 183.358961 \nL 336.072564 183.315457 \nL 338.108441 183.515052 \nL 339.12638 183.477352 \nL 340.144318 183.680219 \nL 340.144318 183.680219 \n\" style=\"fill:none;stroke:#348abd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 225.542909 \nL 20.5625 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 355.3625 225.542909 \nL 355.3625 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 225.542909 \nL 355.3625 225.542909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 8.102909 \nL 355.3625 8.102909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 224.040625 45.459159 \nL 348.3625 45.459159 \nQ 350.3625 45.459159 350.3625 43.459159 \nL 350.3625 15.102909 \nQ 350.3625 13.102909 348.3625 13.102909 \nL 224.040625 13.102909 \nQ 222.040625 13.102909 222.040625 15.102909 \nL 222.040625 43.459159 \nQ 222.040625 45.459159 224.040625 45.459159 \nz\n\" style=\"fill:#e5e5e5;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;stroke-width:0.5;\"/>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 226.040625 21.201346 \nL 246.040625 21.201346 \n\" style=\"fill:none;stroke:#e24a33;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_36\"/>\n    <g id=\"text_17\">\n     <!-- Generator loss -->\n     <defs>\n      <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(254.040625 24.701346)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"139.013672\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"202.392578\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"263.916016\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"305.029297\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"366.308594\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"405.517578\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"466.699219\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"507.8125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"539.599609\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"567.382812\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"628.564453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"680.664062\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_37\">\n     <path d=\"M 226.040625 35.879471 \nL 246.040625 35.879471 \n\" style=\"fill:none;stroke:#348abd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_38\"/>\n    <g id=\"text_18\">\n     <!-- Discriminator Loss -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <g transform=\"translate(254.040625 39.379471)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"104.785156\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"156.884766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"211.865234\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"252.978516\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"280.761719\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"378.173828\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"405.957031\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"469.335938\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"530.615234\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"569.824219\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"631.005859\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"672.119141\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"703.90625\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"757.869141\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"819.050781\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"871.150391\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pae7947eae5\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"8.102909\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "losses_g = [] # store generator loss after each epoch\n",
    "losses_d = [] # store discriminator loss after each epoch\n",
    "images = []   # store images generatd by the generator\n",
    "\n",
    "\n",
    "# create the noise vector\n",
    "noise = create_noise(SAMPLE_SIZE)\n",
    "# put the networks in training mode\n",
    "gen_net.train()\n",
    "disc_net.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_g = 0.0\n",
    "    loss_d = 0.0\n",
    "    for idx, (img, _) in enumerate(train_loader):\n",
    "        # move image to gpu if exists\n",
    "        img = img.to(device)\n",
    "        # get batch size\n",
    "        b_size = len(img)\n",
    "        # run the discriminator for DISC_STEPS number of steps\n",
    "        for _ in range(DISC_STEPS):\n",
    "            fake_x = gen_net(create_noise(b_size))\n",
    "            real_x = img\n",
    "            # train the discriminator network\n",
    "            loss_d += train_discriminator(real_x, fake_x)\n",
    "        fake_x = gen_net(create_noise(b_size))\n",
    "        # train the generator network\n",
    "        loss_g += train_generator(fake_x)\n",
    "    # create the final fake image for the epoch\n",
    "    generated_img = gen_net(noise).cpu().detach()\n",
    "    # make the images as grid\n",
    "    generated_img = make_grid(generated_img)\n",
    "    # save the generated torch tensor models to disk\n",
    "    save_generator_image(generated_img, f\"gan_outputs/gen_img{epoch}.png\")\n",
    "    images.append(generated_img)\n",
    "    epoch_loss_g = loss_g / idx # total generator loss for the epoch\n",
    "    epoch_loss_d = loss_d / idx # total discriminator loss for the epoch\n",
    "    losses_g.append(epoch_loss_g)\n",
    "    losses_d.append(epoch_loss_d)\n",
    "    \n",
    "    print(f\"Epoch {epoch} of {EPOCHS}\")\n",
    "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")\n",
    "\n",
    "print('DONE TRAINING')\n",
    "torch.save(gen_net.state_dict(), 'gan_outputs/generator.pth')\n",
    "torch.save(disc_net.state_dict(), 'gan_outputs/discriminator.pth')\n",
    "\n",
    "# save the generated images as GIF file\n",
    "imgs = [np.array(to_pil_image(img)) for img in images]\n",
    "imageio.mimsave('gan_outputs/generator_images.gif', imgs)\n",
    "\n",
    "# plot and save the generator and discriminator loss\n",
    "plt.figure()\n",
    "plt.plot(losses_g, label='Generator loss')\n",
    "plt.plot(losses_d, label='Discriminator Loss')\n",
    "plt.legend()\n",
    "plt.savefig('gan_outputs/loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}